{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"KnBATwudXQuR","colab_type":"code","outputId":"a34a6742-c5bd-4c2d-efbc-233678bcb908","executionInfo":{"status":"ok","timestamp":1552875976642,"user_tz":-540,"elapsed":723,"user":{"displayName":"Taeho Lee","photoUrl":"https://lh3.googleusercontent.com/-7O6EFXPdFgk/AAAAAAAAAAI/AAAAAAAABX8/udoe9nQYTjo/s64/photo.jpg","userId":"14667176613406605256"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"cell_type":"code","source":["import numpy as np\n","\n","class MDP:\n","\n","    def __init__(self,P,R,discount):\n","        '''\n","        P -- Transition probability matrix: |S| x |A| x |S'|\n","        R -- Reward function: |S| x |A|\n","        discount -- discount factor: scalar in [0,1)\n","        '''\n","\n","        self.nStates = P.shape[0]\n","        self.nActions = P.shape[1]\n","        self.P = P\n","        self.R = R\n","        self.discount = discount\n","        \n","    def valueIteration(self,initialV,nIterations=np.inf,tolerance=0.01):\n","        '''\n","        initialV -- Initial value function: array of |S| entries\n","        nIterations -- limit on the # of iterations: scalar (default: infinity)\n","        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n","\n","        Outputs:\n","        V -- Value function: array of |S| entries\n","        '''\n","        #initialize\n","        V = initialV\n","        prevV = np.zeros(self.nStates)\n","        \n","        #repeat\n","        while True:\n","            prevV = V.copy()\n","            delta = 0\n","            for s in range(self.nStates):\n","                # V[s] <-- max_a R^a[s] + gamma sum_s' P^a[s,s'] prevV[s]\n","                # V[s] = max(R[s]) + self.discount * sum(P[s] * prevV[s])\n","                V[s] = np.max(np.add(self.R[s], self.discount * np.dot(self.P[s], prevV)))\n","                delta = max(delta, abs(V[s] - prevV[s]))\n","            print('V = ',V)\n","            if delta < tolerance:\n","                break\n","        \n","        return V\n","\n","    def extractPolicy(self,V):\n","        '''최적 가치 함수로부터 (deterministic) 최적 정책 추출\n","        pi <-- argmax_a R^a + gamma P^a V\n","        Inputs:\n","        V -- Value function: array of |S| entries\n","        Output:\n","        policy -- Policy: array of |S| entries'''\n","        policy = np.zeros(self.nStates) - 1\n","        for s in range(self.nStates):\n","            # 다음 줄을 수정\n","            policy[s] = np.argmax(np.add(self.R[s], self.discount * np.dot(self.P[s], V)))\n","        return policy\n","\n","# main\n","# DP 모델 정의\n","# Transition probability matrix: |S| x |A| x |S'|\n","P = np.array([[[0,1,0,0],[0,0,1,0]],[[0,1,0,0],[0,0,0,1]],[[0,0,0,1],[0,0,1,0]],[[0,0,0,1],[0,0,0,1]]])\n","\n","# Reward function: |S| x |A|\n","R = np.array([[-1,1],[-2,3],[-1,-2],[0,0]])\n","\n","# Discount factor: scalar in [0,1]\n","discount = 1\n","\n","# MDP object 생성\n","mdp = MDP(P,R,discount)\n","\n","# Value iteration 테스트\n","V = mdp.valueIteration(initialV=np.zeros(mdp.nStates))\n","print('V = ',V)\n","policy = mdp.extractPolicy(V)\n","print('policy =',policy)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["V =  [ 1.  3. -1.  0.]\n","V =  [ 2.  3. -1.  0.]\n","V =  [ 2.  3. -1.  0.]\n","V =  [ 2.  3. -1.  0.]\n","policy = [0. 1. 0. 0.]\n"],"name":"stdout"}]},{"metadata":{"id":"wcgAlQSAEQ-2","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}